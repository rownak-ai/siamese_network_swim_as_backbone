# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NngH42LuP0i9OQ4tN6G0DMB92oeSipWk
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import torch.nn.functional as F
from timm import create_model
import os
from PIL import Image
from torch.utils.data import Dataset
import random
import ssl
import certifi

class SwinTransformerEncoder(nn.Module):
    def __init__(self, pretrained=True):
        super(SwinTransformerEncoder, self).__init__()
        ssl._create_default_https_context = ssl._create_unverified_context
        self.swin_transformer = create_model('swin_base_patch4_window7_224', pretrained=pretrained)
        self.head = nn.Identity()
        self.num_features = self.swin_transformer.num_features
        for param in self.swin_transformer.parameters():
            param.requires_grad = False

    def forward(self, x):
        x = self.swin_transformer(x)
        return x


class SiameseNetwork(nn.Module):
    def __init__(self, encoder):
        super(SiameseNetwork, self).__init__()
        self.encoder = encoder
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(1000, 4096),
            nn.Sigmoid()
        )

    def forward(self, x1, x2=None):
        x1 = self.encoder(x1)
        x1 = self.fc(x1)

        if x2 is not None:
            x2 = self.encoder(x2)
            x2 = self.fc(x2)
            return x1, x2
        else:
            return x1  # Return only the embedding for a single image


def triplet_loss(anchor, positive, negative, margin=1.0):
  
  distance_negative = F.pairwise_distance(anchor, negative, 2)
  distance_positive = F.pairwise_distance(anchor, positive, 2)
  losses = torch.relu(distance_positive - distance_negative + margin)
  return losses.mean()


class TripletCustomData(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.data = []
        self.targets = []
        self.class_to_idx = {}
        self.idx_to_class = {}

        # Load data and targets from the root directory
        for class_name in os.listdir(root_dir):
            class_path = os.path.join(root_dir, class_name)
            if os.path.exists(class_path):
                self.class_to_idx[class_name] = len(self.class_to_idx)
                self.idx_to_class[len(self.idx_to_class)] = class_name
                for filename in os.listdir(class_path):
                    file_path = os.path.join(class_path, filename)
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        self.data.append(file_path)
                        self.targets.append(self.class_to_idx[class_name])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        anchor_data_path, anchor_target = self.data[index], self.targets[index]
        anchor_data = Image.open(anchor_data_path)

        positive_indices = [i for i, target in enumerate(self.targets) if target == anchor_target and i != index]
        if positive_indices:
            positive_idx = random.choice(positive_indices)
            positive_data_path, positive_target = self.data[positive_idx], self.targets[positive_idx]
            positive_data = Image.open(positive_data_path)
        else:
            positive_data = anchor_data

        negative_indices = [i for i, target in enumerate(self.targets) if target != anchor_target]
        negative_idx = random.choice(negative_indices)
        negative_data_path, negative_target = self.data[negative_idx], self.targets[negative_idx]
        negative_data = Image.open(negative_data_path)

        if self.transform is not None:
          anchor_data = self.transform(anchor_data)
          positive_data = self.transform(positive_data)
          negative_data = self.transform(negative_data)

        return anchor_data, positive_data, negative_data

def train(model, train_loader, optimizer, criterion, num_epochs, device):
    best_loss = float('inf')
    for epoch in range(num_epochs):
        running_loss = 0.0
        for batch_idx, (data_anchor, data_positive, data_negative) in enumerate(train_loader):
            data_anchor, data_positive, data_negative = data_anchor.to(device), data_positive.to(device), data_negative.to(device)
            optimizer.zero_grad()
            anchor, positive = model(data_anchor, data_positive)
            anchor, negative = model(data_anchor, data_negative)
            loss = criterion(anchor, positive, negative, margin=1.0)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')

        if running_loss / len(train_loader) < best_loss:
            best_loss = running_loss / len(train_loader)
            torch.save(model.state_dict(), 'D:\\Siamese Network for Traffic Sign Classification\\best_model.pth')  # Save the best model


def evaluation(model, val_loader, device):
    correct = 0
    total = 0

    with torch.no_grad():
        for anchor_data, anchor_positive, anchor_negative in val_loader:
            anchor_data, anchor_positive, anchor_negative = anchor_data.to(device), anchor_positive.to(device), anchor_negative.to(device)
            anchor, positive = model(anchor_data, anchor_positive)
            anchor, negative = model(anchor_data, anchor_negative)
            distance_positive = F.pairwise_distance(anchor, positive, 2)
            distance_negative = F.pairwise_distance(anchor, negative, 2)

            correct += torch.sum(distance_positive < distance_negative).item()
            total += anchor_data.size(0)

    accuracy = correct / total
    return accuracy


def compute_embeddings(model, dataset, device):
  """
  Computes and stores class embeddings for the dataset.

  Args:
      model: The Siamese Network model.
      dataset: The TripletCustomData instance representing the dataset.
      device: The device to use (CPU or GPU).

  Returns:
      dict: A dictionary containing class labels as keys and their embeddings as values.
  """
  embeddings_dict = {}
  model.eval()
  with torch.no_grad():
    for image_path, target in zip(dataset.data, dataset.targets):
      image = Image.open(image_path)
      image_tensor = dataset.transform(image).unsqueeze(0).to(device)
      embedding = model(image_tensor)  # Get the embedding for the image
      embeddings_dict[dataset.idx_to_class[target]] = embedding.cpu()  # Store with class label
  return embeddings_dict


def predict_class(image_path, embeddings_dict, dataset):
  """
  Predicts the class of a single image using the Siamese Network.

  Args:
      image_path: Path to the image for classification.
      embeddings_dict: Dictionary containing class labels as keys and their embeddings as values.

  Returns:
      str: Predicted class label.
  """
  image = Image.open(image_path)
  image_tensor = dataset.transform(image).unsqueeze(0).to(device)

  with torch.no_grad():
    embedding = model(image_tensor)  # Get the embedding for the single image

    # Find the class with the most similar embedding (replace with your chosen metric)
    max_similarity = -float('inf')
    predicted_class = None
    for class_label, class_embedding in embeddings_dict.items():
      similarity = F.cosine_similarity(embedding, class_embedding).item()
      if similarity > max_similarity:
        max_similarity = similarity
        predicted_class = class_label

  return predicted_class

def load_model(model_path, device):
  """
  Loads the Siamese Network model from a saved state dictionary.

  Args:
      model_path: Path to the saved model state dictionary (.pth) file.
      device: The device to use (CPU or GPU).

  Returns:
      SiameseNetwork: The loaded model.
  """
  model = SiameseNetwork(SwinTransformerEncoder(pretrained=False))  # Set pretrained=False for loading weights
  model.load_state_dict(torch.load(model_path, map_location=device))
  model.to(device)
  model.eval()  # Set to evaluation mode for prediction
  return model


if __name__ == '__main__':
  # Hyperparameters (adjust as needed)
  learning_rate = 0.0001
  num_epochs = 50

  # Device configuration
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  # Define data transformations
  data_transforms = transforms.Compose([
      transforms.Resize((224, 224)),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
  ])

  # Prepare datasets
  train_dataset = TripletCustomData(root_dir='D:\\Siamese Network for Traffic Sign Classification\\custom_folder\\train', transform=data_transforms)
  val_dataset = TripletCustomData(root_dir='D:\\Siamese Network for Traffic Sign Classification\\custom_folder\\val', transform=data_transforms)

  # Data loaders
  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)
  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)

  # Model
  model = SiameseNetwork(SwinTransformerEncoder(pretrained=True))
  model.to(device)

  # Optimizer and criterion
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  criterion = triplet_loss

  # Train the model
  train(model, train_loader, optimizer, criterion, num_epochs, device)

  model_path = 'D:\\Siamese Network for Traffic Sign Classification\\best_model.pth'
  loaded_model = load_model(model_path, device)

    # Optional Evaluation
  accuracy = evaluation(loaded_model, val_loader, device)
  print(f'Validation Accuracy: {accuracy:.4f}')

  # Pre-compute class embeddings (replace with a function call if needed)
  embeddings_dict = compute_embeddings(loaded_model, train_dataset, device)

  # Prediction on a single image (example usage)
  image_path_2 = 'D:\\Siamese Network for Traffic Sign Classification\\custom_folder\\val\\Danger_Bend_Right\\Danger_Bend_Rightimage_1624135279062803.png'
  predicted_class = predict_class(image_path_2, embeddings_dict, train_dataset)
  print(f"Predicted class for single image: {predicted_class}")
  


